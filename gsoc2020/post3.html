<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Please delete this script if you use this HTML. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-7580334-1');
  </script>
  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <title>Surya S Dwivedi</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <name>Post 3</name>
              </p>
		<p align="center">
                <heading>July 24th, 2020</heading>
              </p>
              <p>
The past weeks went into implementing the CNN3D and MaxPool3D backward pass functions. This was the most challenging part of the project so far. For both layers, the backward pass functions takes as input the backward_gradients of the next layer(i.e. gradient of loss wrt to next layer input), and computes the gradients wrt to input for the current layer, using chain rule. It also computes the gradients wrt any of the parameters within this layer(weights/biases). When the high-level backward is called for the entire deep-net, it starts from the last layer and calls layer-specific backward pass for each layer. For each layer-specific backward pass function, input is the gradient wrt to input of the next layer, and it return gradients wrt to input of the current layer, which is then passed to the previous layer(as it is the output for the previous layer) and so on till we reach the input. This way we compute the gradient wrt to any of the weights/biases in our network, and then finally do the update step. 
For CNN_3D we have weights and biases for the kernel. So the gradient is calculated for these as well in addition to the input for the Conv3D layer. For MaxPool3D layer, we have no weights and biases so the gradient is computed only for the input to this layer. A lot of blog posts explain how these gradients can be computed for 2D Conv/MaxPool, mathematically(i.e. using chain rule), and I extended those to 3D by intuition, but sometimes I had to redo the math for 3D when something was less obvious. 
		</p> 
		</td>
        
          </tr>
        </table>

</body>

</html>

